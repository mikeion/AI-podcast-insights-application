{"podcast_details": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "BloombergGPT - an LLM for Finance with David Rosenberg - #639", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right, everyone. Welcome to another episode of the TwinMall AI Podcast. I am your host, Sam Charrington. And today I'm joined by David Rosenberg. David is head of the machine learning strategy team in the office of the CTO at Bloomberg. Before we get into today's conversation, be sure to take a moment to head over to Spotify, Apple Podcasts, or your listening platform of choice. And if you enjoy the show, please leave us your best rating and review. David, welcome to the podcast. Thank you. Great to be back. It is amazing to think that it's just over what five years ago that you were on the show. Yeah, it's been a while. World has changed in the AI scene. Absolutely. You were just mentioning that at that time we were doing just audio only shows. Now we're doing this, of course, in audio and video. It can be seen on YouTube. But I think that's probably the least of the changes that we've seen, especially in this NLP space. What do you think? Yeah, I'll say. I mean, I think for me, when GPT-3 came out, it was about three years ago, that was a moment of, wow, this is different. This is something that seems really new. The capabilities were really impressive. Absolutely. So in that vein of GPT, today we're going to be talking about your project, which is Bloomberg GPT, and how you trained and built large language models specializing in financial language. Before we jump into that, share a little bit about your background just to refresh those who weren't listening in five years ago. Five years ago, right. See, academically, I studied math in undergrad. In college, I was already interested. I'd heard about neural networks. I wanted to do a project on neural networks. Just a funny anecdote. My stochastic processes instructor said, don't do neural... This was in 2000, don't do neural networks. Those are kind of passe, do these Bayesian networks. So I took a detour into probabilistic modeling for quite some time. Anyway, I went to grad school in statistics focusing in machine learning and moved out to New York City, where I was at a startup for many years and eventually joined Bloomberg, where I'm in the CTO office working on machine learning strategy, heading the machine learning strategy team. Awesome. Tell us a little bit about the background of Bloomberg GPT. I mean, in one sense, it seems like an obvious thing to try to build, but what was the need that drove you to try to create that? It's not necessarily so obvious to build it. That's kind of one of the things that we do in the CTO office is make these strategic decisions on what to invest in, with both money and time and people's resources, in particular on new machine learning technology. That's what our group would be focusing on. So as I mentioned, I guess it was mid 2020 when GPT-3 came out. And the question was, is this a direction we pursue, we invest in? Because it was clearly a big investment, right? Everyone knew how much GPT-3, we didn't know how much GPT-3 actually cost me, but it was clear that it was a huge investment. And we decided that it was worth making the move. Maybe there's some risk there, but it seems like the possibilities were pretty great. So that was kind of a decision made back in late 2020 to start building towards this goal of our own GPT-3 style model. And I'm not sure we knew exactly at that time what it would be used for. We're still experimenting to figure out how best to use it for our purposes. Can you provide an overview of it and its capabilities? Sure. Well, in some ways, it's a general purpose model. It's also, we see kind of purpose built for finance applications. So our training data set was a split between the standard general purpose training data that are used for building, we don't know exactly what chat GPT models use, but things like OPT and Bloom, we know the data sets that they used. And we kind of followed a lot of their training data plan for about half of our data set. And then we had another half of the data set that is based on Bloomberg's data collected over many years, starts in 2007. That's called Finpile. So about half of our training data was finance specific Bloomberg curated data. And digging into that a bit more, are we talking about kind of financial reports or articles or I think I remember the last time we spoke, that conversation was focused on kind of extracting information from semi-structured documents. Did some of that come into play? What was the makeup of Finpile? Very much a mixture of all the things you mentioned. So a big chunk of it is news data that we have, like news that we have acquired. It's financial filings, that's like company filings, press releases, transcripts of various calls that we have. It's kind of a broad collection of financial data. Got it. Some of those documents have tables and charts in them. So we didn't do any new additional processing specifically for this training data set. But when that information was already extracted, we used it. Is there anything to be done kind of taking advantage of the semi-structured nature of a lot of that information? You've got a lot of metadata in addition to raw text. Right. So we didn't do anything specifically different for the first structured data. It was kind of tokenized the same way as everything else. But one thing we did have some concern about or some focus on was numerical data because finance data has a lot of numbers in it. And we were concerned with the way that I guess a lot of previous work used the GPT-2 tokenizer, which doesn't treat numbers in any special way. So you'll get tokens, long number like 5234 may tokenize into 523 and then four, or it may tokenize into 52 and then 34. And we were concerned that that makes it, it feels like a harder problem for the model to solve when it doesn't have any kind of consistent representation of numbers. So we took a stab at it following kind of the Palm model also did this, where we broke up numbers into individual digits. So maybe the model could then figure out the idea of first digit is the highest order and the second digit is the second highest order and so on. So that's kind of one thing we did to accommodate this financial data, having a lot of numbers in it. So talk a little bit about the process of training the model. It's for the size that you achieved. There are few out there who have attempted that and there are a lot of, I'm imagining there was a lot of kind of new ground to explore. Is that the case? Yes. I think we're still at the stage where it's always kind of an adventure training one of these very large models. When we started our process, there was OPT model for Meta and they did a very nice job of sharing their kind of training logs. All the challenges that they went through, they were very detailed describing them and what they did to resolve them. And so we kind of had that as a bit of a roadmap. And then Bloom also published the Bloom model from Big Science Hugging Face also published their training logs, training Chronicles. That was about it as far as detailed information on how to handle the types of issues that come up when you're actually doing the training. Kind of because of that, we want to control our risk by making the model as close as we could to something that we knew worked before. And so in our case, we copied the Bloom model architecture fairly closely with some small tweaks, the tokenizer being one of them. I mentioned the way we handled numbers being a key piece of that. One or two other spots where we change, I could talk about that later as well. So the training process starts and we call it V0 because there are subsequent versions. And one innovation or thing we tried in the first version of the model was something that seemed innocuous enough, which was when we noted that in our data, in the Bloomberg data set we call it FinPile, all the documents are timestamped. And the data further back is not as good quality for whatever reason, there's less up it. And moreover, it seemed like the most recent data would have the most correct and accurate data factually speaking. And so it seemed reasonable to do something you could call a curriculum learning or whatever it is. It's not a purely random mix shuffle of the training data. We decided to order the training data kind of sequentially in time for the part that's FinPile for the Bloomberg piece. The other data, which is about half of it, was randomly shuffled. So we start training and as for our validation data to measure progress, we are using the month following all that training data. So the most recent data would be our validation data. So we're training for four or five days getting started and we notice the training performance curve levels off, the validation performance curve levels off, or at least it slows down a lot. We're looking at it, is it actually still learning or has it stopped learning? It's hard to tell. And after about eight or 10 days, we decided to stop that training. And we were worried that maybe the curriculum learning line of sorting by timestamp of the document was fine, but didn't seem to be working. And it seemed like that might've been a step too far into the unknown. And so we started over, got rid of that curriculum learning. So we randomly shuffled all the data. And that was kind of the beginning of version one of training. Started off better for about, I think eight days or so, nice improvement of the training performance and valuation performance. We also were tracking the norm of the gradient, which is a spot where you often see issues like, so the gradient norm will suddenly spike up or gradually increase. And we're looking out for that. And sure enough, after something like eight days, we see the gradient norm almost angle upwards and kind of start linearly climbing and couldn't figure out what it was from. And unfortunately, it also coincided with the validation performance kind of getting worse and spiking. So that's when we turned to the limited amount of roadmap we have from OPT paper, we also encountered these kind of gradient norm spikes. We started, not only was there a gradual increase in the gradient norm, there are also these spikes in gradient norm, which corresponded with poor performance. And they had all kind of this recipe they used to get past these spikes, which was kind of roll back to a checkpoint, maybe a hundred steps before the spike. You reshuffle your training data, maybe lower your learning rate. That's kind of the recipe. We tried that and didn't seem to help that much in the sense that we weren't getting kind of nice, compelling learning curves that are like, improving performance gradually gradually. But then we dug into the weights, you know, with these gradient norm spikes, what's happening to the weights? We found something really interesting, which to this day, don't really know the source of this. It's kind of an open question. There's 70 layers in our network. And the only thing that seemed off was in the very first layer. The very first thing in the first layer is this layer norm and the scale weights of that layer norm, right when the gradient norm started increasing, those parameters went from decreasing to steadily increasing. It's very strange. All the weights were more or less throughout the model, more or less kind of on average staying the same magnitude and these layer norm scales were going down and then up. So the down piece we think was a bit of a bug in how we were using our optimizer. We were applying a weight decay, which pulls the weights towards zero to scale weights, which should be centered around one. So that was a little bug that we fixed in version two. It's kind of a thorough call through our code and we found a few other things involving mixed precision. For whatever reason, the Bloom model either didn't need to or just didn't use the full 32-bit precision in certain parts of the model where often one does use them because the lower precision weights are not precise enough. We put that back in. We decided to put an extra layer norm at the very beginning before the first layer as kind of additional protection and we started over again, this time with the shuffled data and that was version two. This was finally the version of the model that worked. We ran for 42 days with nice steady decrease before we ran into some challenges, which was basically that the model stopped learning after about 75% of our data set. We tried a bunch of stuff, but really we're kind of towards the end of our budget and nothing seemed to be working compellingly. So we kind of called it and said, this is a performance and downstream task is already good, more than met our expectations and hopes. We still have a chunk of training data to apply to later when we have some more time and ideas. And yeah, that was the training experience. Oh wow. So 42 days was around the total time of training? 42 days until we started kind of leveling off and then we ran it for another week or so. Okay. Can you talk a little bit about the team that was involved in pulling this together? You mentioned budget. I'm curious about that. I'm curious about infrastructure, what you use there. Talk through some of those things. Sure. So the team was ultimately about nine people. Some were hands down coding, about four were doing kind of actual implementation work. Three of those were focused on the machine learning data aspects. One was focused on kind of optimization and compute aspects. The rest of us were in some sense advisory, but it was calling the literature for things that might help fix the models and working on evaluation and those sorts of things. So compared to a lot of these large models, we had a relatively small team. Yeah. What else? As far as hardware infrastructure, we are training on Amazon SageMaker. We use their SMP platform for optimization. On Nvidia, we're using 40 gigabyte A100 GPUs. We had 512 of them. What else? Software, PyTorch. Our model was kind of small tweaks from the Bloom model, Source Code, which I mentioned. And can you talk about cost? So I don't know exactly. The way it worked from a big picture is that we bought in advance some number of hours on these GPUs. I think it was like 1.3 million GPU hours. I don't know offhand. I think you could find the rack rate for that. I think we had some negotiated rate for a certain amount of hours in a certain period of time. I'm not sure exactly. Okay. So you've got this model. You've trained it up. You mentioned you had some folks working on validation. Talk through validation and how you evaluate the performance of the model. Right. So there's two pieces of that. There's kind of evaluation while we're training and then evaluation after the fact. The while we are training evaluation evolved a bit over time. I started with just this validation set from the last month of training data, which I mentioned earlier. As we were going, we added in another validation set that was chosen randomly from the training period, which is a little bit more comforting because that would have the same distribution as the training distribution, whereas this last month may be different. So it's a little bit harder to interpret. As we got further into it, we had actual downstream tasks that we evaluate on. There's MMLU, which is kind of a giant multiple choice question, task covering knowledge from like a wide range of areas, pretty hard questions. And then there's BBH, Big Bench Hard, which we took kind of the multiple choice questions from. Those are the things that we were monitoring while we were training. Afterwards we did kind of a much more thorough analysis of performance. We had, I guess one category is internal and external tasks and another is rather like kind of Bloomberg specific. These are our tasks for our downstream needs and then the general publicly available tasks. And within those, they're kind of financial specific and generic ones. So kind of our, we consider a cohort of models to compare to, we looked at OPT, the 66 billion parameter model. Ours is 50. So that was kind of the closest of the OPT family and the Bloom model and the GPT Neo-X model from Eleuther. They're open source. Those are the ones that we could run on anything. And then other models will publish in their papers their performance on some benchmarks. So when we could, we pulled in kind of reported numbers. So that's how we got the GPT-3 numbers we compared to in the paper, the Palm numbers we compared to. And the results were really good. We were kind of at least competitive with Bloom and OPT on most tasks and probably were better more often than not. But certainly as far as general purpose tasks, we were in the same ballpark as the others in this cohort. And on financial benchmarks? Financial data, we were significantly better. And one possible reason, there's probably two reasons and I don't know how to allocate to each, but one is the data we trained on. It was half-bitless financially sourced. And perhaps the tokenizer approach helped. It's hard to know. We didn't do the ablation studies to determine that. As far as what we would do next, that's one thing that we'd focus on, I think, is more experimentation on these small changes and maybe on a smaller model size to know what makes a difference and what doesn't. And did you already have or did you need to create a kind of a financial benchmark for evaluation? The internal... So things external exist. One that I like is a conf fin QA. It's kind of like reading comprehension for financial documents. There's a text, there's at least one table of financial information and then some question the question of numerical reasoning. So that exists, that we performed very well. That's one where the gap between our model and other models was maybe most impressive. So we do a lot of sentiment analysis on various sources. So there's news and social media and kind of company statements. And so we have kind of internal benchmark data sets for that sort of thing for our own internal tasks. Another interesting thing that we'll do is NED, Named Entity Disambiguation. So you're reading a news article, it mentions Apple, being able to map that to our knowledge base where Apple is a well-defined company. So for example, you can map it to its stock ticker or something. So this would be a task that would be kind of finance specific. And that was one of our internal benchmarks that we did well on, that we're pleased with our performance on that. Also this is not so much a formal benchmark because it's more generative and it is more challenging to evaluate these things. But internal Bloomberg, there's something called a, it's called BQL, Bloomberg Query Language. You could think of it as an SQL, but maybe much more complicated or capable for combining and operating on all the data internal to Bloomberg, a large piece of that data. It's difficult to use. So could it translate a natural language request into this Bloomberg Query Language? Now it wasn't trained on Bloomberg Query Language at all. And so we tried this in kind of a few shot setting where we gave some examples. So it would see BQL in the prompt and it did rather well. It did rather impressively that it kind of could pick up the sense of this language from 10 shots or something. We also played around with news headline generation, given an article, can you come up with a good headline automatically? So these are kind of things we were experimenting with in a less formal basis because they're just harder to evaluate numerically. Got it. What's the context size that the model has? 2048 tokens, 2048 tokens. I'm curious, maybe stepping back a little bit, given how quickly the space is evolving, what's your meta take on, is it worth doing an effort like this? You mentioned you spent in excess of a million dollars, models, the baseline against Bloom, your baseline if you were to choose to do this today would probably be different and your comps would be different. Was it worth it from a learning exercise? Was it worth it as a practical, hey, we can use this or we can get a year of use out of this thing and it's better than what we had as an alternative? Who would you recommend take this on? What's your feeling on all that? Yeah, great questions. So one question that we wrestled with a lot internally was should we build from scratch or taking an existing model and fine tune it? But it turns out there actually aren't that many open source models that you can take the weights, fine tune for commercial use of the scale that we're considering. I think we just saw the first of those last week in the Falcon model, right? Yeah, so that's an exciting contribution. I have not looked into the details of that. So when we started there just weren't a lot of options, presumably more will come up. And so the question will become more and more relevant, do we fine tune? And I think it's unclear still, in our case, for example, what would we be fine tuning on? We have like 350 billion tokens in Finpile. That's like a full data set. Is it as effective to kind of take an existing model and then fine tune it on essentially, I guess you could almost call it domain adaptation where it's not fine tuning, it's like completely kind of pushing the distribution somewhere else. That may end up being the right approach. But I think the science on that is not even figured out. It's almost harder. There's almost more options on how you fine tune it than to train from scratch. And if you have a very large data set to fine tune on, it's not clear that there's a massive financial difference. There's also the question of to just train a small model, a smaller model that's just on very domain specific information. So there's certainly examples of that in the literature that people speak about. And maybe that's another approach. To advise on people training from scratch, the route we made, I mean, for us, it was a great learning experience. And we're continuing on this path. There's certain another option that I don't know that you mentioned is kind of using existing models as an API, like sending data to OpenAI or other companies that do the same. And that's a step too far for us because there's certain data that we just don't want to send away. We want to handle it in-house. We don't want to have that process at this point by other people's systems. So some people are going to have concerns about wanting to keep data in-house on-prem. It's a tough call. It's a large investment to build a model from scratch, but it gives you more options. Yeah. And you mentioned that you followed very closely the Loom architecture and you took a lot out of their training notes, kind of suggesting that the things you run into in training are very specific to the architecture that you're using. Do you feel like the things that you learned are transferable to kind of the next model architecture that you might want to explore? Or do you feel kind of locked into building on this current architecture? No, we don't feel locked in. A lot of the things you learn are what you instrument in your model training and just kind of building up the infrastructure or knowing what infrastructure you want to build for testing the model as it goes and that sort of thing. I think we have a lot of work to do still on data set selection. We didn't follow Bloom's data set because we brought in a lot of our own stuff. There's questions on cleaning, which seemed to be really important. Also I guess for a while there is a sense that it's really best to train on every data item once. You don't want to repeat documents. And it's not so clear how important that is at this point. And so the relative weightings of the different portions of the data set. Wikipedia seems like a great source of knowledge. Do you run that through 10 times as opposed to, we kind of had it three times because it happened to show up in various collections that we used. Those are kind of problems that apply to any architecture. Tokenizer is another really interesting piece of the puzzle that's orthogonal to architecture. I mentioned a little bit with digits, but there's big questions on how big should your vocabulary size be. One other change we made from most models is our vocabulary size was about 150,000. Typical English only model is like 50,000. So that's another spot where we deviated from most model approaches. And we did not have time to do extensive studies on smaller models to see like pros and cons. And the learnings that we did on that would also kind of are kind of independent of architecture. A lot of the conversation in the broader around LLMs more broadly is around there are two, I guess two numbers, kind of the number of parameters and the context length. Either of those things at the top of your list of things you would do differently or things that are lacking, like that feel like big constraints for you or are the things that you just mentioned like exploring smaller models, all these other things, are those like more interesting to you to play around with? Is it just scale or is it all these other kind of nuances that you've been describing? Yeah. I mean, there's definitely, there's a lot of stuff going there. It's not just scale. The Llama paper was pretty interesting because their smaller models did great. And it seems like it was largely due to, they were able to, they continued training for a trillion tokens or something on the smaller models, way more than Chinchilla optimal for the smaller model sizes. So as we think about what's happening next, you are asking, is it bigger models, smaller? In some sense, we a little bit want to experiment more with the smaller models for a while because they're so much easier to use. You can run inference on a smaller model on a single GPU, whereas at 50 billion parameter model, you need a kind of an inference platform to use it because it's got to go across multiple GPUs. So there is some interest in seeing how much we can get out of the smaller models, maybe the llama style training. There's clear performance differences between different models of the same size. And so it's a matter of collecting those different tricks and secret sauces that may kind of get the best use out of the data you have for your kind of compute budget. So we have a lot to learn there and that's part of our research right now. There's a relationship between train from scratch versus fine tune versus context. Because a lot of times you want this large context so you can stick in a lot of context that isn't in the base model, but you've trained from scratch on the kind of data that you work with. Have you found then that you don't need a large context because of that? Or do you still find yourself wanting more tokens to be able to construct your prompts? Right, right, right. So how I think about that is the fact that we have this fin pile in the data set that puts in knowledge into the model and maybe kind of ways to deal with certain types of document. There's a step that always happens after the initial training, instruction tuning, it's typical where you kind of condition the model to answer questions of a certain type. And before you do the instruction tuning, if you wanted to solve a problem it hasn't seen or a question type it hasn't seen in the training data, you kind of give it a bunch of examples of exactly that question and you continue training it on that sort of thing. And that kind of takes the place of this multi-shot learning, which when you do multi-shot, you may want a big context window so you can put a lot of examples, put a lot of information in. So instruction tuning kind of gets rid of that need to some extent. But there's other things you'd want a giant context window for, which is just new information. So you have a new document, it's 70 pages long, you want to be able to interact with it, query it, what does it say, reason with the model about it. I don't see a way around having a very large context window for that sort of thing. But it's an interesting point you're making, like do we not need a larger window for things like few-shot learning because of the training on fin pile and stuff? Yeah, maybe so. Maybe that helps. And can you talk a little bit about the instruction tuning process? What was your model there? That's ongoing. And we're using a combination of publicly available data sets for that, like Flan. And it certainly wasn't created with the idea of being used for instruction tuning, but we do have a lot of internal data that's of the sort that we can use for this sort of thing for instruction tuning to prep the model for the types of tasks we have. And can you elaborate on that? Is this like... So last time we spoke, I mentioned we have a huge team, thousands of people working on Bloomberg data. And so a lot of it is like kind of annotation tasks. So here's a paragraph, identify the key entities being mentioned, that'd be a named entity recognition task, so to speak. And so the way we store this data, this is kind of like labeled data that can be repurposed as instruction tuning. So you kind of have this label data set and you kind of reformat it into queries and responses, essentially. Exactly. Right. Because the original training data, it's not in that format. It's just documents. It's not like find the key company mentioned in this paragraph. So that's what the instruction tuning does, or this label data has and that we use for instruction tuning. And so did you find then between the public instruction sets and this reformatted internal data that you did not need to do any custom instruction tuning data set development? Oh, I see. In terms of like asking... Like asking new questions, getting new experts. Yeah. Yeah, we haven't done that yet. Okay. So we haven't had to yet. But that's certainly a possible direction. Yeah, it seems like that could be useful. More data can always seems... It can be helpful. Does anything stand out as a... I was going to ask about kind of limitations or challenges, but I mean, we've talked a lot about that. I guess I'm wanting like a netted out. These are the top, like top end things that we think are still open issues for you and like the order in which you think you need to address them. So when we built our model, there's definitely a time constraint. And we skipped a lot of steps that would have been really helpful had we had more time for the areas where we deviated from the traditional work to gain some conviction that that was a good change. For example, the way we had a new tokenizer and the data splits we used, rather the composition of the data we used. We've got a lot of confidence that we can do it. We can build this big model and it's useful and it's good. Now that we have this confidence and we're kind of going to double down our investment, let's kind of start back again a bit. Let's work on experiments at a smaller scale. We're going to try out a variety of tokenizations, a variety of data mixes and data pre-processing and architectural choices and be more disciplined on that experimentation. That's a step that we kind of had to skip by necessity that is very important. And so I think going back to step one with the knowledge you already have is a big thing that we're going to jump into next. And then we can scale back up. And as I mentioned earlier, we want both small models and larger models for practical use. On the topic of practical use, you mentioned these internal tasks. What's the process? How do you think about kind of productionizing something like this? Is this in production, on the path to production? Is it a learning experiment and you're using other things for your actual internal tasks? Where are you and all that? So nothing from this model is in production yet. It's all research and experimentation. We're certainly trying it out, variety of directions. Can it help us solve existing problems that we already have solutions for? Can it help us solve them in a better way or with less investment in training data, for example? We're also really interested in new use cases. As I mentioned, this natural language to Beach Well, we'd love to kind of have an internal code assistant that knows our libraries and that sort of thing. I mentioned put in a large document, being able to interact with it, ask it what information it contains, that sort of thing. In some sense, we're very open-minded and working a lot of directions to see where this can have the most impact. As far as production use, we need to be very cautious about it. No one has solved the hallucination problem. These language models say wrong things, do wrong things, do strange things. So there's got to be kind of a process around it that makes it safe to use, either just internally for clients at some point in the future potentially. And have you started digging into what that process might look like and or how to address some of these issues or are you waiting for kind of the broader research to catch up on kind of the core issues? Well, we're starting with internal usage. And for that, it's not as much an issue of kind of safety and reputation as it is function. Is it useful? Does it do the job? And that's what we're focusing on right now. But you've got to be thinking about like if you're doing internal tasks and people come to rely on it, they kind of become less discriminative in its answers. And like, you've got to have some system for them to kind of check the work or this thing. And if they're doing that, should they just do the other thing in the first place? Like it's a bunch of interesting questions in there, I would think. That's a very interesting question. Yeah. If you need a human oversight, but if the AI is so good, how do you keep the human engaged? Yeah. That's a really interesting issue. Have you started thinking at all about kind of ethical considerations beyond kind of the brand risks that you've mentioned that come into play in fielding models like this? Ethical issues in terms of, could you elaborate on that? Do you mean like that the model might say something offensive or exuding bias or... Yeah, those kinds of things. I mean, yeah, we're absolutely concerned about that. I think we're definitely kind of monitoring the literature and the broader discussion on things like that. So we're certainly attentive to that area, but I think keeping things internal and also frankly focusing on issues of finance topics and code completion and kind of basic summarization tasks intuitively that keeps us a bit apart from a lot of the most obvious issues that would come up, but certainly concerned, yeah. Awesome. Well, super interesting project, David, and it's been great to reconnect and learn a little bit about how you've tackled it. Yeah, thanks. Great chatting again. Thanks so much for the time. Thank you. All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit twiMLAI.com. Of course, if you like what you hear on the podcast, please subscribe, rate, and review the show on your favorite podcatcher. Thanks so much for listening and catch you next time."}, "podcast_summary": "In this podcast episode, David Rosenberg, Head of the Machine Learning Strategy Team at Bloomberg, discusses the development of Bloomberg GPT, a large language model specializing in financial language. The project started in late 2020 following the release of GPT-3, and the aim was to build a model that could be used for various finance applications. The training data consisted of a mix of general purpose data and finance-specific data from Bloomberg. The training process involved several challenges, including issues with the gradient norm and the scale weights of the layer normalization in the model. Despite these challenges, the model performed well on various tasks, both general purpose and finance-specific. The team is now focused on experimentation with smaller models and further exploration of tokenization, data selection, and architectural choices. The model is currently being used for research and experimentation, with plans for future potential production use alongside careful considerations of ethical and safety issues.", "podcast_guest": {"name": "David Rosenberg", "org": "Bloomberg", "title": "", "summary": "Harry and David, LLC (Harry and David) is an American-based premium food and gift producer and retailer. The company sells its products through direct mail, online, corporate gifting, and in their flagship location in Medford, Oregon, and operates the brands Harry & David, Wolferman's, and Vital Choice. Harry & David was founded in 1910 by Samuel Rosenberg as Bear Creek Orchards in Medford, Oregon, as a premium fruit company. As of 2014, it is owned by 1-800-Flowers."}, "podcast_highlights": "- Highlight 1 of the podcast: \"So when we started our process, there was OPT model for Meta and they did a very nice job of sharing their kind of training logs.\"\n- Highlight 2 of the podcast: \"There's a relationship between train from scratch versus fine-tune versus context.\"\n- Highlight 3 of the podcast: \"What I think about that is the fact that we have this fin-pile in the data set that puts in knowledge into the model.\"\n- Highlight 4 of the podcast: \"We have a lot to learn there and that's part of our research right now.\"\n- Highlight 5 of the podcast: \"So we have a lot of work to do still on data set selection.\""}